---
title: "Tidymodels live session - EcoDataScience"
author: "Gavin McDonald - Environmental Markets Solutions Lab (emLab)"
date: "11/19/2019"
output: html_notebook
---

# Introduction 

This live session has been adapted from from Edgar Ruiz's [A Gentle Introduction to tidymodels](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/). It will run through an end-to-end modeling example using the `mtcars` dataset. 

I will briefly touch on important steps of the predictive modeling process, but this is *not* mean to be a comprehensive instruction on predictive modeling (*a.k.a.* machine learning). This tutorial is rather just meant to give a flavor of what's possible using the new `tidymodels` universe of packages. For a much more detailed overview, see Max Kuhn's fantastic [Applied Predictive Modeling](http://appliedpredictivemodeling.com).

# Install necessary pacakges

```{r}
# pacman will help us install any necessary packages
if (!require("pacman")) install.packages("pacman")
# pacman::p_load checks to see if this packages are installed, and installs them if not
pacman::p_load(tidymodels, ranger, randomForest)
```


# Load tidymodels

Load the tidymodels library. This loads a collection of both tidymodels packages, and select tidyverse packages like dplyr, purrr, ggplot2.

```{r}
library(tidymodels)
```

# Data sampling 

The first step in the modeling process is to split your data into separate training and testing datasets. The model will be trained using the training dataset, and the testing dataset will not be used until you are ready to assess model performance. The `rsample::initial_split` function helps with this initial splitting of the data. The `rsample` includes many other helpful functions for splitting the data for cross-validation, bootstrapping, etc.

```{r}
# Split the cars dataset, using 75% of the data for training and 25% for testing
cars_split <- initial_split(mtcars, prop = 0.75)

# This rsplit object tells you how many observations are used for training, how many for testing, and how many total
cars_split

# The training function can be used to extract the training data from the rsplit object
cars_split %>%
  training()

# The testing function can be used to extract the testing data from the rsplit object
cars_split %>%
  testing()
```

# Data pre-processing 

After splitting the data, we will do some data processing. To do this, we will use the `recipe` package. A recipe is a blueprint for how data will be processed. By creating a blueprint, rather than processing data directly, we can apply the same blueprint to training and testing datasets. Importantly, the recipe is defined using only data from the training dataset, which will allow us to see how well the model performs using the testing dataset. Recipe steps can be defined using pipes with a number of sequential steps - there are many many options for recipe steps.

```{r}

cars_recipe <- training(cars_split) %>%
  recipe(mpg ~.) %>%
  # step_corr removes highly correlated variables
  step_corr(all_numeric()) %>%
  # step_center normalizes data to have a mean of 0
  step_center(all_numeric(), -all_outcomes()) %>%
  # step_scale normalizes data to have a standard deviation of 0
  step_scale(all_numeric(), -all_outcomes()) %>%
  # prep trains the recipe using the training dataset
  prep()

cars_recipe

# Use use the juice function to apply the prepped recipe to the training dataset
cars_training <- juice(cars_recipe)

cars_training

# We use the bake function to apply the prepped recipe to the testing dataset
cars_testing <- cars_recipe %>%
  bake(testing(cars_split)) 

cars_testing
```

# Model training

Next, we will use the `parsnip` package to define a number of models. Generally, we use parsnip to define 3 things about our model:

1. The type of model (e.g., linear regression or random forest)  
2. the mode of the model (e.g., regression or classification)  
3. The engine for the model (e.g., `ranger` or `randomForest`) 

After we've defined the model in this way, we can use the `fit` function to fit the model.  

```{r}
# Define and fit a linear regression model
cars_model_lm <- linear_reg() %>%
  set_engine("lm") 

cars_model_lm

cars_fit_lm <- cars_model_lm %>%
  fit(mpg ~ ., data = cars_training)

cars_fit_lm

# Define and fit a random forest regression model using the randomForest engine/package
cars_model_randomForest <-  rand_forest(trees = 100, mode = "regression") %>%
  set_engine("randomForest") 

cars_model_randomForest

cars_fit_randomForest <- cars_model_randomForest%>%
  fit(mpg ~ ., data = cars_training)

cars_fit_randomForest

# Define and fit a random forest regression model using the ranger engine/package
cars_model_ranger <- rand_forest(trees = 100, mode = "regression") %>%
  set_engine("ranger") 

cars_model_ranger

cars_fit_ranger <- cars_model_ranger %>%
  fit(mpg ~ ., data = cars_training)

cars_fit_ranger
```

Once we have the model fits, we can use the `predict` function to generate our predictions for our testing dataset. The predict function always produces a dataframe with the same number of rows as observations. Because of this, `bind_cols` can be used to bind the predictions to the original dataframe

```{r}
# Generate predictions for our testing using the ranger model
predict(cars_fit_ranger, cars_testing)

# Add these ranger predictions to the testing dataset
cars_fit_ranger %>%
  predict(cars_testing) %>%
  bind_cols(cars_testing)

# Save this combined dataframe for later
cars_ranger_predict <- cars_fit_ranger %>%
  predict(cars_testing) %>%
  bind_cols(cars_testing) %>%
  # Add a column for model name
  mutate(model_name = "ranger")

# Let's do the same thing for the linerar regression model
cars_lm_predict <- cars_fit_lm %>%
  predict(cars_testing) %>%
  bind_cols(cars_testing) %>%
  mutate(model_name = "lm")

# Let's do the same thing for the randomForest model
cars_randomForest_predict <- cars_fit_randomForest %>%
  predict(cars_testing) %>%
  bind_cols(cars_testing) %>%
  mutate(model_name = "randomForest")

# Let's combine all of these datasets so we can look at them side-by-side
cars_all_predict <- bind_rows(cars_lm_predict,
                              cars_ranger_predict,
                              cars_randomForest_predict)
```

Let's just look and see how our predictions line up with the observed values in our testing dataset.

```{r}
cars_all_predict %>%
  ggplot(aes(x = mpg,y=.pred,color=model_name)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Observed MPG",
       y = "Predicted MPG",
       title = "Predictions vs observed values for 3 model types\nA simple linear regression is overlaid")
```


# Model validation

Using our predictions from the `parsnip` package, we can use the `yardstick` package to generate model performance metrics. This can be done using the `metrics` function, which generates a default metric set (for a regression model, these are root mean squared error or `rsme`, r-squared or `rsq`, and mean absolute error or `mae`; for a classification model, these are `accuracy` and Kappa or `kap`). You can also define a custom set of metrics using `metric_set`, and there are also individual functions for all metric types.

```{r}
cars_ranger_predict %>%
  # Here we use the metric functions and must define the truth value and the estimated prediction
  metrics(truth = mpg, estimate = .pred)
```

When the predictions are in a dataframe, we can group by model type and calculat metrics by group

```{r}
cars_all_predict%>%
  group_by(model_name) %>%
  metrics(truth = mpg, estimate = .pred)

cars_all_predict%>%
  group_by(model_name) %>%
  metrics(truth = mpg, estimate = .pred)%>%
  ggplot(aes(x = model_name, y = .estimate)) +
  geom_bar(stat="identity") +
  facet_wrap(.~.metric,scales="free") +
  labs(x = "Model name",
       y = "Model performance metric estimate",
       title = "Model performance metrics for 3 model types")
```

We can also do what we just did in a much more tidy fashion, while also keeping the model specifications, model fits, model predictions, and model metrics all in a single dataframe. This ensures that things stay together, and makes it very easy to extract summary statistics or plots. `purrr::map` makes this all possible

```{r}
# Define a tibble using model names and their associated specifications
all_models <- 
  tibble(model_name = "lm",
                     model = list(cars_model_lm)) %>%
  add_row(model_name = "ranger",
                     model = list(cars_model_ranger)) %>%
  add_row(model_name = "randomForest",
                     model = list(cars_model_randomForest))

all_model_results <- all_models %>%
  # Add a column for model fits
  mutate(model_fit = purrr::map(model,
                         ~fit(.x, mpg ~ ., data = cars_training)),
         # Add a column for predictions
         model_predictions = purrr::map(model_fit,
                              ~.x %>% 
                                predict(cars_testing) %>%
                                bind_cols(cars_testing)),
         # Add a column for model metrics
         model_metrics = purrr::map(model_predictions,
                             ~metrics(.x, truth = mpg, estimate = .pred)))

all_model_results

# This plot is the same as the one we made above
all_model_results %>%
  unnest(model_metrics) %>%
  ggplot(aes(x = model_name, y = .estimate)) +
  geom_bar(stat="identity") +
  facet_wrap(.~.metric,scales="free") +
  labs(x = "Model name",
       y = "Model performance metric estimate",
       title = "Model performance metrics for 3 model types")
```

